{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86f0bf6c-3832-4bb5-b4ae-08c1bab83aab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: opencv-python in /home/user/conda/lib/python3.7/site-packages (4.5.5.62)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/user/conda/lib/python3.7/site-packages (from opencv-python) (1.21.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python\n",
    "%pip install huggingface-hub\n",
    "%pip install diffusers\n",
    "%pip install segment-anything\n",
    "%pip install lama-cleaner\n",
    "%pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a316121-7443-4cbe-a388-82985f0a8771",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: opencv-python in /home/user/conda/lib/python3.7/site-packages (4.5.5.62)\n",
      "Requirement already satisfied: matplotlib in /home/user/conda/lib/python3.7/site-packages (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/user/conda/lib/python3.7/site-packages (from opencv-python) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/user/conda/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/user/conda/lib/python3.7/site-packages (from matplotlib) (9.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/user/conda/lib/python3.7/site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/conda/lib/python3.7/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/conda/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/user/conda/lib/python3.7/site-packages (from matplotlib) (4.28.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user/conda/lib/python3.7/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-rqs99ot7\n",
      "  Running command git clone --filter=blob:none -q https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-rqs99ot7\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.imgenv-unruffled-lehmann-0/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[?25hmkdir: cannot create directory ‘images’: File exists\n",
      "--2023-07-28 13:28:19--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 271475 (265K) [image/jpeg]\n",
      "Saving to: ‘images/truck.jpg.4’\n",
      "\n",
      "truck.jpg.4         100%[===================>] 265.11K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-07-28 13:28:19 (2.35 MB/s) - ‘images/truck.jpg.4’ saved [271475/271475]\n",
      "\n",
      "--2023-07-28 13:28:21--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 168066 (164K) [image/jpeg]\n",
      "Saving to: ‘images/groceries.jpg.4’\n",
      "\n",
      "groceries.jpg.4     100%[===================>] 164.13K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2023-07-28 13:28:21 (1.89 MB/s) - ‘images/groceries.jpg.4’ saved [168066/168066]\n",
      "\n",
      "--2023-07-28 13:28:22--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 52.222.236.74, 52.222.236.115, 52.222.236.65, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|52.222.236.74|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
      "Saving to: ‘sam_vit_h_4b8939.pth.1’\n",
      "\n",
      "sam_vit_h_4b8939.pt 100%[===================>]   2.39G  59.6MB/s    in 42s     \n",
      "\n",
      "2023-07-28 13:29:05 (58.8 MB/s) - ‘sam_vit_h_4b8939.pth.1’ saved [2564550879/2564550879]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install opencv-python matplotlib\n",
    "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "\n",
    "!mkdir images\n",
    "!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
    "!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n",
    "\n",
    "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b62d77-ef36-43e9-806a-a7807ca9c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5404055b-0f26-49ee-940d-8774ab94892a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1+cu117\n",
      "Torchvision version: 0.14.1+cu117\n",
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import pycocotools._mask as mask_tool\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import argparse\n",
    "\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec6ff43-0573-4e48-9b73-ff4d174ca97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_dir = 'photos/'\n",
    "allowed_categories = [ \n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "    6,\n",
    "    7,\n",
    "    8\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e34ece2c-6256-4340-a710-407ec706d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read JSON file\n",
    "def parse_json(file_path):\n",
    "  jfile = open(file_path)\n",
    "  file_data = json.load(jfile)\n",
    "  res = []\n",
    "  for s in file_data['annotations']:\n",
    "    img_id = s['image_id']\n",
    "    category_id = s['category_id']\n",
    "    s_bbox = s['bbox']\n",
    "    box = np.array([s_bbox[0], s_bbox[1], s_bbox[0]+s_bbox[2], s_bbox[1]+s_bbox[3]])\n",
    "    \n",
    "    res.append({\n",
    "      \"image_id\": img_id, \n",
    "      \"category_id\": category_id, \n",
    "      \"bbox\": [int(box[0]), int(box[1]), int(box[2]), int(box[3])], \n",
    "      \"score\": 1\n",
    "  })\n",
    "\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec184da1-4990-451a-889b-ab470a94ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_json(file_path):\n",
    "    jfile = open(file_path)\n",
    "    file_data = json.load(jfile)\n",
    "    res = []\n",
    "    for s in file_data['annotations']:\n",
    "      img_id = s['image_id']\n",
    "      category_id = s['category_id']\n",
    "      s_bbox = s['bbox']\n",
    "      box = np.array([s_bbox[0], s_bbox[1], s_bbox[0]+s_bbox[2], s_bbox[1]+s_bbox[3]])\n",
    "      \n",
    "      res.append({\n",
    "        \"image_id\": img_id, \n",
    "        \"category_id\": category_id, \n",
    "        \"bbox\": [int(box[0]), int(box[1]), int(box[2]), int(box[3])], \n",
    "        \"score\": 1\n",
    "    })\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cbf70b3-cb19-41af-8400-b70089979d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bboxes(img_name, texts, img_id, category_id, marked_bbox):\n",
    "  processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "  model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "\n",
    "  image = Image.open(photos_dir + img_name)\n",
    "\n",
    "  inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "  target_sizes = torch.Tensor([image.size[::-1]])\n",
    "  results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)\n",
    "\n",
    "  # img = cv2.imread(photos_dir + img_name)\n",
    "  img = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)\n",
    "  i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "  res = []\n",
    "  text = texts[i]\n",
    "  font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "  boxes, scores, labels = results[0][\"boxes\"], results[0][\"scores\"], results[0][\"labels\"]\n",
    "  print(\"max score is \", torch.argmax(scores))\n",
    "  for box, score, label in zip(boxes, scores, labels):\n",
    "      box = [int(i) for i in box.tolist()]\n",
    "      print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "      res.append({\n",
    "      \"image_id\": img_id, \n",
    "      \"category_id\": category_id, \n",
    "      \"bbox\": [int(box[0]), int(box[1]), int(box[2]), int(box[3])], \n",
    "      \"score\": round(score.item(), 3)\n",
    "  })\n",
    "      img = cv2.rectangle(img, [int(box[0]), int(box[1])], [int(box[2]), int(box[3])], (255,0,0), 5)\n",
    "      img = cv2.rectangle(img, [int(marked_bbox[0]), int(marked_bbox[1])], [int(marked_bbox[2]), int(marked_bbox[3])], (0,0,255), 3)\n",
    "      if box[3] + 25 > 768:\n",
    "          y = box[3] - 10\n",
    "      else:\n",
    "          y = box[3] + 25 \n",
    "  \n",
    "  # fig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure & 1 axis\n",
    "  # ax.imshow(img[:,:,::-1])\n",
    "  # fig.savefig(f\"result_photos/{img_id}_{category_id}.png\")   # save the figure to file\n",
    "  # plt.close(fig)  \n",
    "  # plt.imshow(img[:,:,::-1])\n",
    "    \n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b018d9f-c95e-445d-a08a-5f4eceb796e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jfile = open('./dataset/annotation_example.json')\n",
    "file_data = json.load(jfile)\n",
    "\n",
    "owlvit_result = []\n",
    "i=0\n",
    "for s in file_data['annotations']:\n",
    "    print(i)\n",
    "    img_id = s['image_id']\n",
    "    category_id = s['category_id']\n",
    "    s_bbox = s['bbox']\n",
    "    marked_bbox = [s_bbox[0], s_bbox[1], s_bbox[0]+s_bbox[2], s_bbox[1]+s_bbox[3]]\n",
    "    img_path = file_data['images'][img_id-1]['file_name']\n",
    "    text = [\"a picture of \" + file_data['categories'][category_id-1]['name']]\n",
    "    \n",
    "    if category_id in allowed_categories:\n",
    "        print('detect ', img_path, text, category_id)\n",
    "        detections = detect_bboxes(img_path, [text], img_id, category_id, marked_bbox)\n",
    "        # print(detections)\n",
    "        \n",
    "        owlvit_result.append(detections)\n",
    "        print(owlvit_result)\n",
    "        print('--------------------')\n",
    "\n",
    "    i+=1\n",
    "    \n",
    "    \n",
    "json_string = json.dumps(owlvit_result)\n",
    "with open('bbox_prediction_result.json', 'w') as f:\n",
    "    f.write(json_string)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0943ed44-0d61-4c25-8db1-19dced337f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))   \n",
    "    \n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "208f5578-3a2e-4880-a10a-571b7e0a7507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM model imported\n"
     ]
    }
   ],
   "source": [
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mask(mask):\n",
    "    binary_mask = mask.cpu().numpy().squeeze().astype(np.uint8)\n",
    "\n",
    "    # Find the contours of the mask\n",
    "    contours, hierarchy = cv2.findContours(binary_mask,\n",
    "                                        cv2.RETR_EXTERNAL,\n",
    "                                        cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Get the largest contour based on area\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Get the new bounding box\n",
    "    bbox = [int(x) for x in cv2.boundingRect(largest_contour)]\n",
    "\n",
    "    # Get the segmentation mask for object \n",
    "    segmentation = largest_contour.flatten().tolist()\n",
    "    \n",
    "    return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation(img_id, category_id, segmentation, annotation_data):\n",
    "    annotation = {\n",
    "        \"image_id\": img_id,\n",
    "        \"category_id\": category_id,\n",
    "        \"segmentation\": segmentation,\n",
    "        \"area\": int(cv2.contourArea(contours[0])),\n",
    "        \"bbox\": [int(x) for x in cv2.boundingRect(contours[0])],\n",
    "        \"iscrowd\": 0,\n",
    "        \"score\": score_bbox\n",
    "    }\n",
    "    \n",
    "    photo = annotation_data['images'][img_id-1]\n",
    "    h, w = photo['height'], photo['width']\n",
    "    print(annotation_data['images'][img_id-1]['file_name'], h, w)\n",
    "    segm = annotation['segmentation']\n",
    "    # print(segm)\n",
    "    if type(segm) == list:\n",
    "        # polygon -- a single object might consist of multiple parts\n",
    "        # we merge all parts into one mask rle code\n",
    "        rles = mask_tool.frPyObjects([segm], h, w)\n",
    "        # print(rles[0])\n",
    "    \n",
    "    annotation['segmentation'] = {\n",
    "        \"size\": rles[0]['size'], \n",
    "        \"counts\": rles[0]['counts'].decode(\"utf-8\")\n",
    "    }\n",
    "    \n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced9104-e124-4042-848f-da785f0219ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start\")\n",
    "jfile = open('dataset/owl-vit_result2.json')\n",
    "file_data = json.load(jfile)\n",
    "\n",
    "jfile = open('dataset/restricted_result.json')\n",
    "img_data = json.load(jfile)\n",
    "\n",
    "segm_result = []\n",
    "i=0\n",
    "for s in file_data:\n",
    "    print(i)\n",
    "    img_id = s['image_id']\n",
    "    category_id = s['category_id']\n",
    "    img_name = img_data['images'][img_id-1]['file_name']\n",
    "    bbox = s['bbox']\n",
    "    score_bbox = s['score']\n",
    "\n",
    "    image = cv2.imread(photos_dir + img_name)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert the bounding box to a tensor\n",
    "    input_box = torch.tensor(bbox, device=predictor.device)\n",
    "    transformed_box = predictor.transform.apply_boxes_torch(input_box, image.shape[:2])\n",
    "\n",
    "    # Set the image\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    # Predict the mask\n",
    "    mask, _, _ = predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_box,\n",
    "        multimask_output=False,\n",
    "    )   \n",
    "\n",
    "    binary_mask = mask.cpu().numpy().squeeze().astype(np.uint8)\n",
    "\n",
    "    # Find the contours of the mask\n",
    "    contours, hierarchy = cv2.findContours(binary_mask,\n",
    "                                        cv2.RETR_EXTERNAL,\n",
    "                                        cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Get the largest contour based on area\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Get the new bounding box\n",
    "    bbox = [int(x) for x in cv2.boundingRect(largest_contour)]\n",
    "\n",
    "    # Get the segmentation mask for object \n",
    "    segmentation = largest_contour.flatten().tolist()\n",
    "\n",
    "    # print(segmentation)\n",
    "        \n",
    "    # Define annotation in COCO format\n",
    "    annotation = {\n",
    "        \"image_id\": img_id,\n",
    "        \"category_id\": category_id,\n",
    "        \"segmentation\": segmentation,\n",
    "        \"area\": int(cv2.contourArea(contours[0])),\n",
    "        \"bbox\": [int(x) for x in cv2.boundingRect(contours[0])],\n",
    "        \"iscrowd\": 0,\n",
    "        \"score\": score_bbox\n",
    "    }\n",
    "    \n",
    "    photo = img_data['images'][img_id-1]\n",
    "    h, w = photo['height'], photo['width']\n",
    "    print(img_data['images'][img_id-1]['file_name'], h, w)\n",
    "    segm = annotation['segmentation']\n",
    "    # print(segm)\n",
    "    if type(segm) == list:\n",
    "        # polygon -- a single object might consist of multiple parts\n",
    "        # we merge all parts into one mask rle code\n",
    "        rles = mask_tool.frPyObjects([segm], h, w)\n",
    "        # print(rles[0])\n",
    "    \n",
    "    annotation['segmentation'] = {\n",
    "        \"size\": rles[0]['size'], \n",
    "        \"counts\": rles[0]['counts'].decode(\"utf-8\")\n",
    "    }\n",
    "    \n",
    "    # print(annotation['segmentation'])\n",
    "    segm_result.append(annotation)\n",
    "    # print(annotation)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18543793-8b30-4771-bbc5-eae95ed0ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = json.dumps(segm_result)\n",
    "\n",
    "with open('dataset/segm_predicted_result.json', 'w') as f:\n",
    "    f.write(json_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c1f78-09ad-4baa-b27f-425f655990d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
