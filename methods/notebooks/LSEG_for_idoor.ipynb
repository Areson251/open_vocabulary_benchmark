{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "7rhiLs62jxKn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lseg (2022)"
      ],
      "metadata": {
        "id": "eBsyrPFPq8Ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download model"
      ],
      "metadata": {
        "id": "7rhiLs62jxKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA5SjMw_q-vd",
        "outputId": "a2e79a5d-387e-46cf-9f68-62f54c0ce27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#backbone VIT-L16 TEXT VIT-B32\n",
        "!gdown 1ayk6NXURI_vIPlym16f_RG3ffxBWHxvb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6p4JkMMJgeq",
        "outputId": "de29c959-3c8d-4c16-ad02-90acd39b0fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ayk6NXURI_vIPlym16f_RG3ffxBWHxvb\n",
            "To: /content/demo_e200.ckpt\n",
            "100% 3.10G/3.10G [00:36<00:00, 85.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vlmaps/vlmaps\n",
        "%cd vlmaps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugoyo7Fpbh7Z",
        "outputId": "650e3df6-3145-4b8d-c0e3-cafc148dc473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vlmaps'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 131 (delta 17), reused 12 (delta 7), pack-reused 102\u001b[K\n",
            "Receiving objects: 100% (131/131), 61.87 MiB | 18.73 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "/content/vlmaps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# CLIP\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "q4TqS_3lGcvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install timm"
      ],
      "metadata": {
        "id": "0NQc7V2pGk6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "kiShTKXtHC96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import math\n",
        "\n",
        "import clip\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "from lseg.additional_utils.models import crop_image, pad_image, resize_image\n",
        "from lseg.modules.models.lseg_net import LSegEncNet\n",
        "from utils.clip_mapping_utils import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TFu0OV1Vbda",
        "outputId": "76a0a05a-f895-4f42-ca21-beb2da314a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__file__:  /content/vlmaps/examples/context.py\n",
            "imported path: /content/vlmaps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## functions"
      ],
      "metadata": {
        "id": "E-6ioLSoj5Dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "from skimage.transform import resize as my_resize\n",
        "\n",
        "def get_bbox(pred):\n",
        "  H, W = pred.shape\n",
        "  rows = np.sum(pred, axis=1)\n",
        "  columns = np.sum(pred, axis=0)\n",
        "  i=0\n",
        "  while  i < H and rows[i]==W:\n",
        "    i+=1\n",
        "  top = i\n",
        "  i=H-1\n",
        "  while i>=0 and rows[i]==W:\n",
        "    i-=1\n",
        "  bottom = i\n",
        "  i=0\n",
        "  while i< W and columns[i]==H:\n",
        "    i+=1\n",
        "  left = i\n",
        "  i=W-1\n",
        "  while i>=0 and columns[i]==H:\n",
        "    i-=1\n",
        "  right = i\n",
        "  if left > right or top > bottom:\n",
        "    return [0, 0, 0, 0]\n",
        "  return [left, top, right-left, bottom-top]\n",
        "\n",
        "def get_lseg_feat(\n",
        "    model: LSegEncNet,\n",
        "    image: np.array,\n",
        "    labels,\n",
        "    transform,\n",
        "    crop_size=480,\n",
        "    base_size=520,\n",
        "    norm_mean=[0.5, 0.5, 0.5],\n",
        "    norm_std=[0.5, 0.5, 0.5],\n",
        "    vis=False,\n",
        "):\n",
        "    vis_image = image.copy()\n",
        "    image = transform(image).unsqueeze(0).cuda()\n",
        "    img = image[0].permute(1, 2, 0)\n",
        "    img = img * 0.5 + 0.5\n",
        "\n",
        "    batch, t, h, w = image.size()\n",
        "    #print(batch, t, h, w)\n",
        "    stride_rate = 2.0 / 3.0\n",
        "    stride = int(crop_size * stride_rate)\n",
        "\n",
        "    long_size = base_size\n",
        "    if h > w:\n",
        "        height = long_size\n",
        "        width = int(1.0 * w * long_size / h + 0.5)\n",
        "        short_size = width\n",
        "    else:\n",
        "        width = long_size\n",
        "        height = int(1.0 * h * long_size / w + 0.5)\n",
        "        short_size = height\n",
        "\n",
        "    cur_img = resize_image(image, height, width, **{\"mode\": \"bilinear\", \"align_corners\": True})\n",
        "\n",
        "    if long_size <= crop_size:\n",
        "        pad_img = pad_image(cur_img, norm_mean, norm_std, crop_size)\n",
        "        #print(pad_img.shape)\n",
        "        with torch.no_grad():\n",
        "            outputs, logits = model(pad_img, labels)\n",
        "        outputs = crop_image(outputs, 0, height, 0, width)\n",
        "    else:\n",
        "        if short_size < crop_size:\n",
        "            # pad if needed\n",
        "            pad_img = pad_image(cur_img, norm_mean, norm_std, crop_size)\n",
        "        else:\n",
        "            pad_img = cur_img\n",
        "        _, _, ph, pw = pad_img.shape  # .size()\n",
        "        assert ph >= height and pw >= width\n",
        "        h_grids = int(math.ceil(1.0 * (ph - crop_size) / stride)) + 1\n",
        "        w_grids = int(math.ceil(1.0 * (pw - crop_size) / stride)) + 1\n",
        "        with torch.cuda.device_of(image):\n",
        "            with torch.no_grad():\n",
        "                outputs = image.new().resize_(batch, model.out_c, ph, pw).zero_().cuda()\n",
        "                logits_outputs = image.new().resize_(batch, len(labels), ph, pw).zero_().cuda()\n",
        "            count_norm = image.new().resize_(batch, 1, ph, pw).zero_().cuda()\n",
        "        # grid evaluation\n",
        "        for idh in range(h_grids):\n",
        "            for idw in range(w_grids):\n",
        "                h0 = idh * stride\n",
        "                w0 = idw * stride\n",
        "                h1 = min(h0 + crop_size, ph)\n",
        "                w1 = min(w0 + crop_size, pw)\n",
        "                crop_img = crop_image(pad_img, h0, h1, w0, w1)\n",
        "                # pad if needed\n",
        "                pad_crop_img = pad_image(crop_img, norm_mean, norm_std, crop_size)\n",
        "                with torch.no_grad():\n",
        "                    output, logits = model(pad_crop_img, labels)\n",
        "                cropped = crop_image(output, 0, h1 - h0, 0, w1 - w0)\n",
        "                cropped_logits = crop_image(logits, 0, h1 - h0, 0, w1 - w0)\n",
        "                outputs[:, :, h0:h1, w0:w1] += cropped\n",
        "                logits_outputs[:, :, h0:h1, w0:w1] += cropped_logits\n",
        "                count_norm[:, :, h0:h1, w0:w1] += 1\n",
        "        assert (count_norm == 0).sum() == 0\n",
        "        outputs = outputs / count_norm\n",
        "        logits_outputs = logits_outputs / count_norm\n",
        "        outputs = outputs[:, :, :height, :width]\n",
        "        logits_outputs = logits_outputs[:, :, :height, :width]\n",
        "    #print(type(outputs))\n",
        "    outputs = outputs.cpu()\n",
        "    outputs = outputs.numpy()  # B, D, H, W\n",
        "    predicts = [torch.max(logit, 0)[1].cpu().numpy() for logit in logits_outputs]\n",
        "    pred = predicts[0]\n",
        "    pred = np.array(cv2.resize(np.array(pred, dtype=float), dsize=(w, h), interpolation=cv2.INTER_CUBIC), dtype=int)\n",
        "    #print(pred)\n",
        "    #print(\"pred\", pred.shape)\n",
        "    bbox = get_bbox(pred)\n",
        "    #print(outputs.shape)\n",
        "    if vis:\n",
        "        new_palette = get_new_pallete(len(labels))\n",
        "        mask, patches = get_new_mask_pallete(pred, new_palette, out_label_flag=True, labels=labels)\n",
        "        seg = mask.convert(\"RGBA\")\n",
        "        # cv2_imshow(vis_image[:, :, [2, 1, 0]])\n",
        "        # cv2.waitKey()\n",
        "        fig = plt.figure()\n",
        "        plt.imshow(seg)\n",
        "        plt.legend(handles=patches, loc=\"upper left\", bbox_to_anchor=(1.0, 1), prop={\"size\": 20})\n",
        "        plt.axis(\"off\")\n",
        "        plt.scatter([bbox[0], bbox[0]+bbox[2]], [ bbox[1],bbox[1]+bbox[3]], color='white')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return outputs, bbox, pred"
      ],
      "metadata": {
        "id": "JAlEH7l2cht_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pix_feats, bbox, pred = get_lseg_feat(model, rgb, ['wall', 'other'], transform, crop_size, base_size, norm_mean, norm_std, vis=True)\n",
        "#print(pix_feats)\n",
        "#pix_feats.shape"
      ],
      "metadata": {
        "id": "uuq6x1jWdNnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
        "\n",
        "import os\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "CuUtdA4fxdOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import PIL\n",
        "\n",
        "def SaveArchive(folder_to_save, where_to_save):\n",
        "  try:\n",
        "      os.mkdir(folder_to_save)\n",
        "  except OSError as error:\n",
        "      pass\n",
        "  fantasy_zip = zipfile.ZipFile('{}.zip'.format(where_to_save), 'w')\n",
        "  for folder, subfolders, files in os.walk(folder_to_save):\n",
        "    for file in files:\n",
        "      fantasy_zip.write(os.path.join(folder, file),\n",
        "                        os.path.relpath(os.path.join(folder, file), folder_to_save),\n",
        "                        compress_type = zipfile.ZIP_DEFLATED)\n",
        "  fantasy_zip.close()\n",
        "\n",
        "def ExtractArchive(filename, where_to_extract):\n",
        "  try:\n",
        "      os.mkdir(where_to_extract)\n",
        "  except OSError as error:\n",
        "      pass\n",
        "  with zipfile.ZipFile(filename, 'r') as zip:\n",
        "    zip.extractall(where_to_extract)\n",
        "\n",
        "def ApplyFunction(original_folder, function, dir_to_save, dir_for_images):\n",
        "  for folder, subfolders, files in os.walk(original_folder):\n",
        "    for file in files:\n",
        "      function(folder+'/'+file, dir_to_save, dir_for_images)\n",
        "\n",
        "def CropImages(ann_file, dir_to_save, dir_for_images):\n",
        "  try:\n",
        "      os.mkdir(dir_to_save)\n",
        "  except OSError as error:\n",
        "      pass\n",
        "  coco = COCO(ann_file)\n",
        "  for key in coco.anns.keys():\n",
        "    img_num = coco.anns[key]['image_id']\n",
        "    image_filename = coco.imgs[img_num]['file_name']\n",
        "    im = PIL.Image.open(\"{}/{}\".format(dir_for_images, image_filename))\n",
        "    box = list(map(int, coco.anns[key]['bbox']))\n",
        "    category_id = coco.anns[key]['category_id']\n",
        "    new_im = im.crop((box[0], box[1], box[0] + box[2], box[1] + box[3])) # (left, top, right, bottom)\n",
        "    # new_im.show()\n",
        "    new_im.save('{}/{}_{}.jpg'.format(dir_to_save, image_filename[:-4], key))\n",
        "    #break"
      ],
      "metadata": {
        "id": "bI6g5eD9xdRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prompt_toolkit.shortcuts.progress_bar.base import E\n",
        "import json\n",
        "import pycocotools._mask as mask_tool\n",
        "\n",
        "def SaveJson(coco):\n",
        "  d={\"annotations\":coco.anns, \"images\":coco.imgs, \"categories\":coco.cats}\n",
        "  with open('/content/new_instances.json', 'w') as f:\n",
        "    json.dump(d, f)\n",
        "\n",
        "def convert_mask(mask):\n",
        "    binary_mask = mask.astype(np.uint8)\n",
        "    #mask.cpu().numpy().squeeze().astype(np.uint8)\n",
        "    # Find the contours of the mask\n",
        "    contours, hierarchy = cv2.findContours(binary_mask,\n",
        "                                        cv2.RETR_EXTERNAL,\n",
        "                                        cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if contours != ():\n",
        "      # Get the largest contour based on area\n",
        "      largest_contour = max(contours, key=cv2.contourArea)\n",
        "      # Get the new bounding box\n",
        "      bbox = [int(x) for x in cv2.boundingRect(largest_contour)]\n",
        "      # Get the segmentation mask for object\n",
        "      segmentation = largest_contour.flatten().tolist()\n",
        "      return segmentation, contours\n",
        "    else:\n",
        "      return {}, contours\n",
        "\n",
        "def get_annotation(img_id, category_id, segmentation, annotation_data, contours):\n",
        "    annotation = {\n",
        "        \"image_id\": img_id,\n",
        "        \"category_id\": category_id,\n",
        "        \"segmentation\": segmentation,\n",
        "        \"area\": int(cv2.contourArea(contours[0])),\n",
        "        \"bbox\": [int(x) for x in cv2.boundingRect(contours[0])],\n",
        "        \"iscrowd\": 0,\n",
        "        \"score\": 1\n",
        "    }\n",
        "    #try:\n",
        "    photo = annotation_data['images'][img_id-1]\n",
        "    #except:\n",
        "    #print(img_id)\n",
        "    #return None\n",
        "    h, w = photo['height'], photo['width']\n",
        "    #print(annotation_data['images'][img_id-1]['file_name'], h, w)\n",
        "    segm = annotation['segmentation']\n",
        "    # print(segm)\n",
        "    if type(segm) == list:\n",
        "        # polygon -- a single object might consist of multiple parts\n",
        "        # we merge all parts into one mask rle code\n",
        "        try:\n",
        "          rles = mask_tool.frPyObjects([segm], h, w)\n",
        "        except:\n",
        "          print(type(segm), type(h), type(w))\n",
        "          print(segm)\n",
        "          return annotation\n",
        "        # print(rles[0])\n",
        "\n",
        "    annotation['segmentation'] = {\n",
        "        \"size\": rles[0]['size'],\n",
        "        \"counts\": rles[0]['counts'].decode(\"utf-8\")\n",
        "    }\n",
        "\n",
        "    return annotation\n",
        "\n",
        "\n",
        "def ApplyModel(model, transform, ann_file, images_dir, allowed_categories=[], show_img=False): #folder_with_annotation,\n",
        "  coco = COCO(ann_file)\n",
        "  segm_pred_res = []\n",
        "  bbox_pred_res = []\n",
        "  jfile = open(ann_file)\n",
        "  annotation_data = json.load(jfile)\n",
        "  for key in coco.anns.keys():\n",
        "    category_id = coco.anns[key]['category_id']\n",
        "    category = coco.cats[category_id]['name']\n",
        "    if category in allowed_categories:\n",
        "\n",
        "      labels = [' '.join(category.split('.')), 'other']\n",
        "      img_num = coco.anns[key]['image_id']\n",
        "      image_path = '{}/{}_{}.jpg'.format(images_dir, coco.imgs[img_num]['file_name'][:-4], key)\n",
        "      bgr = cv2.imread(image_path)\n",
        "      try:\n",
        "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "      except:\n",
        "        continue\n",
        "      _, lseg_bbox, pred  = get_lseg_feat(model, rgb, labels, transform,crop_size=480,\n",
        "                             base_size=520,norm_mean=[0.5, 0.5, 0.5],\n",
        "                             norm_std=[0.5, 0.5, 0.5],vis=show_img)\n",
        "      if lseg_bbox ==[0, 0, 0, 0]:\n",
        "        continue\n",
        "      segmentation, contours = convert_mask(pred)\n",
        "      #coco.anns[key]['lseg_bbox'] = list(lseg_bbox)\n",
        "      #coco.anns[key]['segmentation'] = segmentation\n",
        "      bbox_pred_res.append({\"image_id\": img_num, \"category_id\": category_id,\n",
        "                            \"bbox\": lseg_bbox, \"score\": 1})\n",
        "      if contours!=():\n",
        "        segm_pred_res.append(get_annotation(img_num, category_id,\n",
        "                                          segmentation, annotation_data, contours))\n",
        "      if show_img:\n",
        "        im = PIL.Image.open(image_path)\n",
        "        im.show()\n",
        "  with open('/content/segm_predicted_result_lseg.json', 'w') as f:\n",
        "    json.dump(segm_pred_res, f)\n",
        "  with open('/content/bbox_prediction_result_lseg.json', 'w') as f:\n",
        "    json.dump(bbox_pred_res, f)"
      ],
      "metadata": {
        "id": "wiY1Ip7VHmLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## inference model"
      ],
      "metadata": {
        "id": "VmdYH2IRj-fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cropped_images = 'cropped_images' # path to file with cropped images\n",
        "\n",
        "images = '/content/{}.zip'.format(cropped_images) # name of file with cropped images\n",
        "ExtractArchive(images, '/content/cropped_images')"
      ],
      "metadata": {
        "id": "zgBf7a86xXuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann_file = '/content/indoor.json' # path to annotation file"
      ],
      "metadata": {
        "id": "X4PBYQ8msNds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "all_categories = []\n",
        "coco = COCO(ann_file)\n",
        "for key in coco.cats.keys():\n",
        "  labels.append(' '.join(coco.cats[key]['name'].split('_')))\n",
        "  all_categories.append(coco.cats[key]['name'])\n",
        "lang=','.join(labels)\n",
        "print(labels)\n",
        "print(lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfUlGWwVeX5I",
        "outputId": "035dd9d3-ec8a-4856-9311-20074d2cf50c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.09s)\n",
            "creating index...\n",
            "index created!\n",
            "['carrot', 'cucumber', 'green bell pepper', 'green chili pepper', 'red bell pepper', 'red chili pepper', 'eggplant', 'corn', 'drawer', 'chest of drawers', 'handle', 'box basket case bin container', 'toy cat', 'white toy', 'toy block', 'garlic', 'tomato', 'potato', 'floor', 'wall', 'person', 'key', 'gripper', 'door', 'poster', 'table', 'ceiling']\n",
            "carrot,cucumber,green bell pepper,green chili pepper,red bell pepper,red chili pepper,eggplant,corn,drawer,chest of drawers,handle,box basket case bin container,toy cat,white toy,toy block,garlic,tomato,potato,floor,wall,person,key,gripper,door,poster,table,ceiling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crop_size = 480  # 480\n",
        "base_size = 520  # 520\n",
        "vis = False\n",
        "\n",
        "# loading models\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "clip_version = \"ViT-B/32\"\n",
        "clip_feat_dim = {\n",
        "    \"RN50\": 1024,\n",
        "    \"RN101\": 512,\n",
        "    \"RN50x4\": 640,\n",
        "    \"RN50x16\": 768,\n",
        "    \"RN50x64\": 1024,\n",
        "    \"ViT-B/32\": 512,\n",
        "    \"ViT-B/16\": 512,\n",
        "    \"ViT-L/14\": 768,\n",
        "}[clip_version]\n",
        "print(\"Loading CLIP model...\")\n",
        "clip_model, preprocess = clip.load(clip_version)  # clip.available_models()\n",
        "clip_model.to(device).eval()\n",
        "lang_token = clip.tokenize(labels) #!!!!!!!!!!!!!!\n",
        "lang_token = lang_token.to(device)\n",
        "with torch.no_grad():\n",
        "    text_feats = clip_model.encode_text(lang_token)\n",
        "    text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
        "text_feats = text_feats.cpu().numpy()\n",
        "model = LSegEncNet(lang, arch_option=0, block_depth=0, activation=\"lrelu\", crop_size=crop_size)\n",
        "model_state_dict = model.state_dict()\n",
        "pretrained_state_dict = torch.load(\"/content/demo_e200.ckpt\")\n",
        "pretrained_state_dict = {k.lstrip(\"net.\"): v for k, v in pretrained_state_dict[\"state_dict\"].items()}\n",
        "model_state_dict.update(pretrained_state_dict)\n",
        "model.load_state_dict(pretrained_state_dict)\n",
        "\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "norm_mean = [0.5, 0.5, 0.5]\n",
        "norm_std = [0.5, 0.5, 0.5]\n",
        "padding = [0.0] * 3\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zXnNkrOeQIl",
        "outputId": "7e92b10e-484c-4af5-c325-60ce78b7d7c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Loading CLIP model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ApplyModel(model, transform, ann_file,\n",
        "                '/content/cropped_images', allowed_categories=all_categories, show_img=False)"
      ],
      "metadata": {
        "id": "tYW0PGowxX7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HBDyeSQfOpAU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}