{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "7rhiLs62jxKn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d2e51b077b654379b7d05d4318bb02aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d7fe507f5eb4095ac1ae0f2575061fa",
              "IPY_MODEL_db35ab070c01441d973f5890794ad81a",
              "IPY_MODEL_24895a0b52644dbaaa6cd3f5ba181d1d"
            ],
            "layout": "IPY_MODEL_c9cfbddd925b4b05976ecd19d1cb6927"
          }
        },
        "1d7fe507f5eb4095ac1ae0f2575061fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a01bba19610941948b7891b225d304d9",
            "placeholder": "​",
            "style": "IPY_MODEL_517eee58d5924da89d894f108e41d44a",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "db35ab070c01441d973f5890794ad81a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37bf6e0b38f496ab0567513573bdd78",
            "max": 1218891162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09b933d343ae442db819f63bcdf9b9c7",
            "value": 1218891162
          }
        },
        "24895a0b52644dbaaa6cd3f5ba181d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efdf50f07ce04109b2742ebcba49a2e9",
            "placeholder": "​",
            "style": "IPY_MODEL_f93237c4d8d7477d8b384e550d9af601",
            "value": " 1.22G/1.22G [00:12&lt;00:00, 160MB/s]"
          }
        },
        "c9cfbddd925b4b05976ecd19d1cb6927": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a01bba19610941948b7891b225d304d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "517eee58d5924da89d894f108e41d44a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d37bf6e0b38f496ab0567513573bdd78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09b933d343ae442db819f63bcdf9b9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "efdf50f07ce04109b2742ebcba49a2e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f93237c4d8d7477d8b384e550d9af601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lseg (2022)"
      ],
      "metadata": {
        "id": "eBsyrPFPq8Ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download model"
      ],
      "metadata": {
        "id": "7rhiLs62jxKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA5SjMw_q-vd",
        "outputId": "cb7bedfd-6ea3-4655-f867-cbd0fd1ddb5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#backbone VIT-L16 TEXT VIT-B32\n",
        "!gdown 1ayk6NXURI_vIPlym16f_RG3ffxBWHxvb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6p4JkMMJgeq",
        "outputId": "4129a1a4-d802-41ca-98e8-ced9f8edd2bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied with the following error:\n",
            "\n",
            " \tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1ayk6NXURI_vIPlym16f_RG3ffxBWHxvb \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vlmaps/vlmaps\n",
        "%cd vlmaps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugoyo7Fpbh7Z",
        "outputId": "12caef62-6d3d-4013-efe2-60a5823ca689"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vlmaps'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 131 (delta 17), reused 12 (delta 7), pack-reused 102\u001b[K\n",
            "Receiving objects: 100% (131/131), 61.87 MiB | 17.81 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "/content/vlmaps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# CLIP\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "q4TqS_3lGcvR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install timm"
      ],
      "metadata": {
        "id": "0NQc7V2pGk6M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "kiShTKXtHC96"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import math\n",
        "\n",
        "import clip\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "from lseg.additional_utils.models import crop_image, pad_image, resize_image\n",
        "from lseg.modules.models.lseg_net import LSegEncNet\n",
        "from utils.clip_mapping_utils import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TFu0OV1Vbda",
        "outputId": "95468c64-55d5-4b0d-f81c-917518a944fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__file__:  /content/vlmaps/examples/context.py\n",
            "imported path: /content/vlmaps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## functions"
      ],
      "metadata": {
        "id": "E-6ioLSoj5Dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "from skimage.transform import resize as my_resize\n",
        "\n",
        "def get_bbox(pred):\n",
        "  H, W = pred.shape\n",
        "  rows = np.sum(pred, axis=1)\n",
        "  columns = np.sum(pred, axis=0)\n",
        "  i=0\n",
        "  while  i < H and rows[i]==W:\n",
        "    i+=1\n",
        "  top = i\n",
        "  i=H-1\n",
        "  while i>=0 and rows[i]==W:\n",
        "    i-=1\n",
        "  bottom = i\n",
        "  i=0\n",
        "  while i< W and columns[i]==H:\n",
        "    i+=1\n",
        "  left = i\n",
        "  i=W-1\n",
        "  while i>=0 and columns[i]==H:\n",
        "    i-=1\n",
        "  right = i\n",
        "  if left > right or top > bottom:\n",
        "    return [0, 0, 0, 0]\n",
        "  return [left, top, right-left, bottom-top]\n",
        "\n",
        "def get_lseg_feat(\n",
        "    model: LSegEncNet,\n",
        "    image: np.array,\n",
        "    labels,\n",
        "    transform,\n",
        "    crop_size=480,\n",
        "    base_size=520,\n",
        "    norm_mean=[0.5, 0.5, 0.5],\n",
        "    norm_std=[0.5, 0.5, 0.5],\n",
        "    vis=False,\n",
        "):\n",
        "    vis_image = image.copy()\n",
        "    image = transform(image).unsqueeze(0).cuda()\n",
        "    img = image[0].permute(1, 2, 0)\n",
        "    img = img * 0.5 + 0.5\n",
        "\n",
        "    batch, t, h, w = image.size()\n",
        "    #print(batch, t, h, w)\n",
        "    stride_rate = 2.0 / 3.0\n",
        "    stride = int(crop_size * stride_rate)\n",
        "\n",
        "    long_size = base_size\n",
        "    if h > w:\n",
        "        height = long_size\n",
        "        width = int(1.0 * w * long_size / h + 0.5)\n",
        "        short_size = width\n",
        "    else:\n",
        "        width = long_size\n",
        "        height = int(1.0 * h * long_size / w + 0.5)\n",
        "        short_size = height\n",
        "\n",
        "    cur_img = resize_image(image, height, width, **{\"mode\": \"bilinear\", \"align_corners\": True})\n",
        "\n",
        "    if long_size <= crop_size:\n",
        "        pad_img = pad_image(cur_img, norm_mean, norm_std, crop_size)\n",
        "        #print(pad_img.shape)\n",
        "        with torch.no_grad():\n",
        "            outputs, logits = model(pad_img, labels)\n",
        "        outputs = crop_image(outputs, 0, height, 0, width)\n",
        "    else:\n",
        "        if short_size < crop_size:\n",
        "            # pad if needed\n",
        "            pad_img = pad_image(cur_img, norm_mean, norm_std, crop_size)\n",
        "        else:\n",
        "            pad_img = cur_img\n",
        "        _, _, ph, pw = pad_img.shape  # .size()\n",
        "        assert ph >= height and pw >= width\n",
        "        h_grids = int(math.ceil(1.0 * (ph - crop_size) / stride)) + 1\n",
        "        w_grids = int(math.ceil(1.0 * (pw - crop_size) / stride)) + 1\n",
        "        with torch.cuda.device_of(image):\n",
        "            with torch.no_grad():\n",
        "                outputs = image.new().resize_(batch, model.out_c, ph, pw).zero_().cuda()\n",
        "                logits_outputs = image.new().resize_(batch, len(labels), ph, pw).zero_().cuda()\n",
        "            count_norm = image.new().resize_(batch, 1, ph, pw).zero_().cuda()\n",
        "        # grid evaluation\n",
        "        for idh in range(h_grids):\n",
        "            for idw in range(w_grids):\n",
        "                h0 = idh * stride\n",
        "                w0 = idw * stride\n",
        "                h1 = min(h0 + crop_size, ph)\n",
        "                w1 = min(w0 + crop_size, pw)\n",
        "                crop_img = crop_image(pad_img, h0, h1, w0, w1)\n",
        "                # pad if needed\n",
        "                pad_crop_img = pad_image(crop_img, norm_mean, norm_std, crop_size)\n",
        "                with torch.no_grad():\n",
        "                    output, logits = model(pad_crop_img, labels)\n",
        "                cropped = crop_image(output, 0, h1 - h0, 0, w1 - w0)\n",
        "                cropped_logits = crop_image(logits, 0, h1 - h0, 0, w1 - w0)\n",
        "                outputs[:, :, h0:h1, w0:w1] += cropped\n",
        "                logits_outputs[:, :, h0:h1, w0:w1] += cropped_logits\n",
        "                count_norm[:, :, h0:h1, w0:w1] += 1\n",
        "        assert (count_norm == 0).sum() == 0\n",
        "        outputs = outputs / count_norm\n",
        "        logits_outputs = logits_outputs / count_norm\n",
        "        outputs = outputs[:, :, :height, :width]\n",
        "        logits_outputs = logits_outputs[:, :, :height, :width]\n",
        "    #print(type(outputs))\n",
        "    outputs = outputs.cpu()\n",
        "    outputs = outputs.numpy()  # B, D, H, W\n",
        "    predicts = [torch.max(logit, 0)[1].cpu().numpy() for logit in logits_outputs]\n",
        "    pred = predicts[0]\n",
        "    pred = np.array(cv2.resize(np.array(pred, dtype=float), dsize=(w, h), interpolation=cv2.INTER_CUBIC), dtype=int)\n",
        "    #print(pred)\n",
        "    #print(\"pred\", pred.shape)\n",
        "    bbox = get_bbox(pred)\n",
        "    #print(outputs.shape)\n",
        "    if vis:\n",
        "        new_palette = get_new_pallete(len(labels))\n",
        "        mask, patches = get_new_mask_pallete(pred, new_palette, out_label_flag=True, labels=labels)\n",
        "        seg = mask.convert(\"RGBA\")\n",
        "        # cv2_imshow(vis_image[:, :, [2, 1, 0]])\n",
        "        # cv2.waitKey()\n",
        "        fig = plt.figure()\n",
        "        plt.imshow(seg)\n",
        "        plt.legend(handles=patches, loc=\"upper left\", bbox_to_anchor=(1.0, 1), prop={\"size\": 20})\n",
        "        plt.axis(\"off\")\n",
        "        plt.scatter([bbox[0], bbox[0]+bbox[2]], [ bbox[1],bbox[1]+bbox[3]], color='white')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return outputs, bbox, pred"
      ],
      "metadata": {
        "id": "JAlEH7l2cht_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pix_feats, bbox, pred = get_lseg_feat(model, rgb, ['wall', 'other'], transform, crop_size, base_size, norm_mean, norm_std, vis=True)\n",
        "#print(pix_feats)\n",
        "#pix_feats.shape"
      ],
      "metadata": {
        "id": "uuq6x1jWdNnD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
        "\n",
        "import os\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "CuUtdA4fxdOL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import PIL\n",
        "\n",
        "def SaveArchive(folder_to_save, where_to_save):\n",
        "  try:\n",
        "      os.mkdir(folder_to_save)\n",
        "  except OSError as error:\n",
        "      pass\n",
        "  fantasy_zip = zipfile.ZipFile('{}.zip'.format(where_to_save), 'w')\n",
        "  for folder, subfolders, files in os.walk(folder_to_save):\n",
        "    for file in files:\n",
        "      fantasy_zip.write(os.path.join(folder, file),\n",
        "                        os.path.relpath(os.path.join(folder, file), folder_to_save),\n",
        "                        compress_type = zipfile.ZIP_DEFLATED)\n",
        "  fantasy_zip.close()\n",
        "\n",
        "def ExtractArchive(filename, where_to_extract):\n",
        "  try:\n",
        "      os.mkdir(where_to_extract)\n",
        "  except OSError as error:\n",
        "      pass\n",
        "  with zipfile.ZipFile(filename, 'r') as zip:\n",
        "    zip.extractall(where_to_extract)\n",
        "\n",
        "def ApplyFunction(original_folder, function, dir_to_save, dir_for_images):\n",
        "  for folder, subfolders, files in os.walk(original_folder):\n",
        "    for file in files:\n",
        "      function(folder+'/'+file, dir_to_save, dir_for_images)\n",
        "\n",
        "def CropImages(ann_file, dir_to_save, dir_for_images):\n",
        "  try:\n",
        "      os.mkdir(dir_to_save)\n",
        "  except OSError as error:\n",
        "      pass\n",
        "  coco = COCO(ann_file)\n",
        "  for key in coco.anns.keys():\n",
        "    img_num = coco.anns[key]['image_id']\n",
        "    image_filename = coco.imgs[img_num]['file_name']\n",
        "    im = PIL.Image.open(\"{}/{}\".format(dir_for_images, image_filename))\n",
        "    box = list(map(int, coco.anns[key]['bbox']))\n",
        "    category_id = coco.anns[key]['category_id']\n",
        "    new_im = im.crop((box[0], box[1], box[0] + box[2], box[1] + box[3])) # (left, top, right, bottom)\n",
        "    # new_im.show()\n",
        "    new_im.save('{}/{}_{}.jpg'.format(dir_to_save, image_filename[:-4], key))\n",
        "    #break"
      ],
      "metadata": {
        "id": "bI6g5eD9xdRs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prompt_toolkit.shortcuts.progress_bar.base import E\n",
        "import json\n",
        "import pycocotools._mask as mask_tool\n",
        "from matplotlib import image as mpimg\n",
        "\n",
        "\n",
        "def SaveJson(coco):\n",
        "  d={\"annotations\":coco.anns, \"images\":coco.imgs, \"categories\":coco.cats}\n",
        "  with open('/content/new_instances.json', 'w') as f:\n",
        "    json.dump(d, f)\n",
        "\n",
        "def convert_mask(mask):\n",
        "    binary_mask = mask.astype(np.uint8)\n",
        "    #mask.cpu().numpy().squeeze().astype(np.uint8)\n",
        "    # Find the contours of the mask\n",
        "    contours, hierarchy = cv2.findContours(binary_mask,\n",
        "                                        cv2.RETR_EXTERNAL,\n",
        "                                        cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if contours != ():\n",
        "      # Get the largest contour based on area\n",
        "      largest_contour = max(contours, key=cv2.contourArea)\n",
        "      # Get the new bounding box\n",
        "      bbox = [int(x) for x in cv2.boundingRect(largest_contour)]\n",
        "      # Get the segmentation mask for object\n",
        "      segmentation = largest_contour.flatten().tolist()\n",
        "      return segmentation, contours\n",
        "    else:\n",
        "      return {}, contours\n",
        "\n",
        "def get_annotation(img_id, category_id, segmentation, annotation_data, contours):\n",
        "    annotation = {\n",
        "        \"image_id\": img_id,\n",
        "        \"category_id\": category_id,\n",
        "        \"segmentation\": segmentation,\n",
        "        \"area\": int(cv2.contourArea(contours[0])),\n",
        "        \"bbox\": [int(x) for x in cv2.boundingRect(contours[0])],\n",
        "        \"iscrowd\": 0,\n",
        "        \"score\": 1\n",
        "    }\n",
        "    #try:\n",
        "    photo = annotation_data['images'][img_id-1]\n",
        "    #except:\n",
        "    #print(img_id)\n",
        "    #return None\n",
        "    h, w = photo['height'], photo['width']\n",
        "    #print(annotation_data['images'][img_id-1]['file_name'], h, w)\n",
        "    segm = annotation['segmentation']\n",
        "    # print(segm)\n",
        "    if type(segm) == list:\n",
        "        # polygon -- a single object might consist of multiple parts\n",
        "        # we merge all parts into one mask rle code\n",
        "        try:\n",
        "          rles = mask_tool.frPyObjects([segm], h, w)\n",
        "        except:\n",
        "          print(type(segm), type(h), type(w))\n",
        "          print(segm)\n",
        "          return annotation\n",
        "        # print(rles[0])\n",
        "\n",
        "    annotation['segmentation'] = {\n",
        "        \"size\": rles[0]['size'],\n",
        "        \"counts\": rles[0]['counts'].decode(\"utf-8\")\n",
        "    }\n",
        "\n",
        "    return annotation\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def MakeMyModel():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  print(device)\n",
        "  crop_size = 480  # 480\n",
        "  base_size = 520  # 520\n",
        "  vis = False\n",
        "\n",
        "  model = LSegEncNet(\"something\", arch_option=0, block_depth=0, activation=\"lrelu\", crop_size=crop_size)\n",
        "  model_state_dict = model.state_dict()\n",
        "  pretrained_state_dict = torch.load(\"/content/drive/MyDrive/demo_e200.ckpt\")\n",
        "  pretrained_state_dict = {k.lstrip(\"net.\"): v for k, v in pretrained_state_dict[\"state_dict\"].items()}\n",
        "  model_state_dict.update(pretrained_state_dict)\n",
        "  model.load_state_dict(pretrained_state_dict)\n",
        "\n",
        "  model.eval()\n",
        "  model = model.to(device)\n",
        "\n",
        "  norm_mean = [0.5, 0.5, 0.5]\n",
        "  norm_std = [0.5, 0.5, 0.5]\n",
        "  padding = [0.0] * 3\n",
        "  transform = transforms.Compose(\n",
        "      [\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "      ]\n",
        "  )\n",
        "  return model, transform\n",
        "\n",
        "\n",
        "def ApplyModel(model, transform, ann_file, images_dir, show_img=False): #folder_with_annotation,\n",
        "  coco = COCO(ann_file)\n",
        "  segm_pred_res = []\n",
        "  bbox_pred_res = []\n",
        "  jfile = open(ann_file)\n",
        "  annotation_data = json.load(jfile)\n",
        "  for key in coco.anns.keys():\n",
        "    category_id = coco.anns[key]['category_id']\n",
        "    category = coco.cats[category_id]['name']\n",
        "    labels = [' '.join(category.split('.')), 'other']\n",
        "    img_num = coco.anns[key]['image_id']\n",
        "    image_path = \"{}/{}\".format(images_dir, coco.imgs[img_num]['file_name'])\n",
        "    bgr = cv2.imread(image_path)\n",
        "    try:\n",
        "      rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "    except:\n",
        "      continue\n",
        "    _, lseg_bbox, pred  = get_lseg_feat(model, rgb, labels, transform,crop_size=480,\n",
        "                            base_size=520,norm_mean=[0.5, 0.5, 0.5],\n",
        "                            norm_std=[0.5, 0.5, 0.5],vis=show_img)\n",
        "    if lseg_bbox ==[0, 0, 0, 0]:\n",
        "      continue\n",
        "    segmentation, contours = convert_mask(pred)\n",
        "    #coco.anns[key]['lseg_bbox'] = list(lseg_bbox)\n",
        "    #coco.anns[key]['segmentation'] = segmentation\n",
        "    bbox_pred_res.append({\"image_id\": img_num, \"category_id\": category_id,\n",
        "                          \"bbox\": lseg_bbox, \"score\": 1})\n",
        "    if contours!=():\n",
        "      segm_pred_res.append(get_annotation(img_num, category_id,\n",
        "                                        segmentation, annotation_data, contours))\n",
        "  with open('/content/segm_predicted_result_lseg.json', 'w') as f:\n",
        "    json.dump(segm_pred_res, f)\n",
        "  with open('/content/bbox_prediction_result_lseg.json', 'w') as f:\n",
        "    json.dump(bbox_pred_res, f)"
      ],
      "metadata": {
        "id": "wiY1Ip7VHmLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03504fd-5ecc-4b83-999f-df1252192b1f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## inference model"
      ],
      "metadata": {
        "id": "VmdYH2IRj-fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = 'rosbag_for_ann_final' # path to file with cropped images\n",
        "\n",
        "images = '/content/{}.zip'.format(images)\n",
        "ExtractArchive(images, '/content/rosbag_for_ann_final')"
      ],
      "metadata": {
        "id": "zgBf7a86xXuW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann_file = '/content/indoor.json' # path to annotation file"
      ],
      "metadata": {
        "id": "X4PBYQ8msNds"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, transform = MakeMyModel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "d2e51b077b654379b7d05d4318bb02aa",
            "1d7fe507f5eb4095ac1ae0f2575061fa",
            "db35ab070c01441d973f5890794ad81a",
            "24895a0b52644dbaaa6cd3f5ba181d1d",
            "c9cfbddd925b4b05976ecd19d1cb6927",
            "a01bba19610941948b7891b225d304d9",
            "517eee58d5924da89d894f108e41d44a",
            "d37bf6e0b38f496ab0567513573bdd78",
            "09b933d343ae442db819f63bcdf9b9c7",
            "efdf50f07ce04109b2742ebcba49a2e9",
            "f93237c4d8d7477d8b384e550d9af601"
          ]
        },
        "id": "XRZyhR659MjX",
        "outputId": "c7cee89c-7ade-497a-e687-205bb70f1340"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 168MiB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2e51b077b654379b7d05d4318bb02aa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ApplyModel(model, transform, ann_file,\n",
        "                '/content/rosbag_for_ann_final/rosbag_for_ann_final', show_img=False)"
      ],
      "metadata": {
        "id": "tYW0PGowxX7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df122e6-8491-4580-9fa6-3ce950a608c3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.11s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HBDyeSQfOpAU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}